GEMINI_API_KEY=sk-5678mnopqrstuvwx5678mnopqrstuvwx5678mnop

# LLM Backend Configuration
# Supported backends: "ollama"
LLM_BACKEND=ollama

# Model selection:
# - For Ollama: phi3:mini, qwen2.5:7b, mistral, etc.
LLM_MODEL=phi3:mini

# Ollama settings (used when LLM_BACKEND=ollama)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Generation parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1024

# Storage
LOG_LEVEL=INFO
STORAGE_DIR=/storage
