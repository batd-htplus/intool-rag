GEMINI_API_KEY=

LLM_BACKEND=ollama

# - For Ollama: phi3:mini, qwen2.5:7b, mistral, etc.
LLM_MODEL=qwen2.5:7b-instruct-q4_K_M

LLM_BASE_URL=http://host.docker.internal:11434

# Generation parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1024

# Storage
LOG_LEVEL=INFO
STORAGE_DIR=/storage
