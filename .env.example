GEMINI_API_KEY=your-gemini-api-key

# LLM Backend Configuration
# Supported backends: "ollama"
LLM_BACKEND=ollama

# Model selection:
# - For Ollama: phi3:mini, qwen2.5:7b, mistral, etc.
LLM_MODEL=phi3:mini

# Ollama settings (used when LLM_BACKEND=ollama)
LLM_BASE_URL=http://host.docker.internal:11434

# Generation parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1024

# Storage
LOG_LEVEL=INFO
STORAGE_DIR=/storage
