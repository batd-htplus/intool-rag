EMBEDDING_PROVIDER_TYPE=http
LLM_PROVIDER_TYPE=http
RERANKER_PROVIDER_TYPE=http

# Embedding model
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_DEVICE=cpu
EMBEDDING_BATCH_SIZE=32

# LLM Backend Configuration
# Supported backends: "ollama", "huggingface"
LLM_BACKEND=ollama

# Model selection - depends on backend:
# - For Ollama: phi3:mini, qwen2.5:7b, mistral, etc.
# - For HuggingFace: microsoft/Phi-3-mini-4k-instruct, Qwen/Qwen2.5-7B-Instruct, etc.
LLM_MODEL=phi3:mini

# Ollama settings (used when LLM_BACKEND=ollama)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# HuggingFace settings (used when LLM_BACKEND=huggingface)
LLM_DEVICE=cpu
MODELS_DIR=./models

# Generation parameters (both backends)
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1024

# Reranker model
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_ENABLED=true
RERANKER_TOP_K=10

# RAG Quality Optimization

# Semantic-aware chunking (enable to improve accuracy)
SEMANTIC_CHUNKING=true
CHUNK_SIZE=512          # Optimal for BGE-M3: 512â€“768 tokens
CHUNK_OVERLAP=50        # Semantic overlap

# Hybrid retrieval configuration
HYBRID_SEARCH_ENABLED=true
BM25_WEIGHT=0.3
VECTOR_WEIGHT=0.7

# Retrieval parameters
RETRIEVAL_TOP_K=10      # Retrieve candidates first
LLM_RELEVANCE_THRESHOLD=0.4

# Cache configuration (significantly improves repeated query speed)
CACHE_EMBEDDINGS=true
CACHE_DIR=/storage/cache

# Embedding instructions (fine-tuned for BGE-M3)
EMBEDDING_QUERY_INSTRUCTION=Represent this sentence for searching relevant passages:
EMBEDDING_PASSAGE_INSTRUCTION=

LOG_LEVEL=INFO

# Storage directory
STORAGE_DIR=/storage


# Production environment (best quality)
SEMANTIC_CHUNKING=true
HYBRID_SEARCH_ENABLED=true
RERANKER_ENABLED=true
CACHE_EMBEDDINGS=true

# Development environment (balanced)
SEMANTIC_CHUNKING=true
HYBRID_SEARCH_ENABLED=true
RERANKER_ENABLED=false
CACHE_EMBEDDINGS=true

# Resource-constrained environment (lightweight)
SEMANTIC_CHUNKING=false
HYBRID_SEARCH_ENABLED=true
RERANKER_ENABLED=false
CACHE_EMBEDDINGS=true
EMBEDDING_BATCH_SIZE=8
